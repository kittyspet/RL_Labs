{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPGPfi8EajGp"
   },
   "source": [
    "Реализуйте алгоритм GAIL на среде Mountain Car. Перед этим сгенерируйте экспертные данные (из детерминированной стратегии с первой практики). Хорошей идеей будет добавить в state (observation) синус и косинус от временной метки t для лучшего обучения."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L1-l3BqFbaUV",
    "ExecuteTime": {
     "end_time": "2025-05-04T17:57:01.040298Z",
     "start_time": "2025-05-04T17:56:59.829145Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import deque\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T17:57:09.952684Z",
     "start_time": "2025-05-04T17:57:09.787282Z"
    }
   },
   "source": [
    "# Создание среды\n",
    "TIME_LIMIT = 250\n",
    "env = gym.wrappers.TimeLimit(\n",
    "    gym.make(\"MountainCar-v0\"),\n",
    "    max_episode_steps=TIME_LIMIT + 1,\n",
    ")\n",
    "\n",
    "# Детерминированная стратегия из первой практики\n",
    "def expert_policy(obs, t):\n",
    "    position, velocity = obs\n",
    "\n",
    "    if t <= 50:\n",
    "        return 0  # Влево\n",
    "    elif t <= 100:\n",
    "        return 2  # Вправо\n",
    "    elif t <= 150:\n",
    "        return 0  # Влево\n",
    "    elif t <= 200:\n",
    "        return 2  # Вправо\n",
    "    else:\n",
    "        return 2  # Вправо\n",
    "\n",
    "# Генерация экспертных данных\n",
    "def generate_expert_data(env, expert_policy, num_episodes=100):\n",
    "    states = []\n",
    "    actions = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        t = 0  # Временная метка\n",
    "\n",
    "        while not done:\n",
    "            # Выбираем действие с помощью экспертной стратегии\n",
    "            action = expert_policy(obs, t)\n",
    "\n",
    "            # Совершаем шаг в среде\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Дополняем состояние синусом и косинусом временной метки t\n",
    "            extended_state = np.append(obs, [np.sin(t), np.cos(t)])\n",
    "            states.append(extended_state)\n",
    "            actions.append(action)\n",
    "\n",
    "            # Обновляем состояние и временную метку\n",
    "            obs = next_obs\n",
    "            t += 1\n",
    "\n",
    "    return np.array(states, dtype=np.float32), np.array(actions, dtype=np.int64)\n",
    "\n",
    "# Генерируем данные\n",
    "print(\"Generating expert data...\")\n",
    "states, actions = generate_expert_data(env, expert_policy, num_episodes=100)\n",
    "\n",
    "# Вывод информации о данных\n",
    "print(\"Количество данных в states:\", len(states))\n",
    "print(\"Количество данных в actions:\", len(actions))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating expert data...\n",
      "Количество данных в states: 17145\n",
      "Количество данных в actions: 17145\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T17:57:13.869229Z",
     "start_time": "2025-05-04T17:57:13.853331Z"
    }
   },
   "source": [
    "def test_policy(env, expert_policy, num_episodes=10):\n",
    "    total_rewards = []\n",
    "    episode_lengths = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        t = 0\n",
    "\n",
    "        while not done:\n",
    "            # Используем экспертную стратегию для выбора действия\n",
    "            action = expert_policy(obs, t)\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            t += 1\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        episode_lengths.append(t)\n",
    "        print(f\"Episode {episode + 1}: Total Reward = {total_reward}, Steps = {t}\")\n",
    "\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    avg_length = np.mean(episode_lengths)\n",
    "    print(f\"Average Reward: {avg_reward:.2f}, Average Episode Length: {avg_length:.2f}\")\n",
    "\n",
    "# Тестируем стратегию\n",
    "print(\"Testing policy...\")\n",
    "test_policy(env, expert_policy, num_episodes=10)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing policy...\n",
      "Episode 1: Total Reward = -186.0, Steps = 186\n",
      "Episode 2: Total Reward = -184.0, Steps = 184\n",
      "Episode 3: Total Reward = -185.0, Steps = 185\n",
      "Episode 4: Total Reward = -184.0, Steps = 184\n",
      "Episode 5: Total Reward = -185.0, Steps = 185\n",
      "Episode 6: Total Reward = -184.0, Steps = 184\n",
      "Episode 7: Total Reward = -184.0, Steps = 184\n",
      "Episode 8: Total Reward = -185.0, Steps = 185\n",
      "Episode 9: Total Reward = -184.0, Steps = 184\n",
      "Episode 10: Total Reward = -184.0, Steps = 184\n",
      "Average Reward: -184.50, Average Episode Length: 184.50\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wyLq9c-2bvXE",
    "ExecuteTime": {
     "end_time": "2025-05-04T17:57:16.335499Z",
     "start_time": "2025-05-04T17:57:16.332275Z"
    }
   },
   "source": [
    "# Размерности для среды MountainCar\n",
    "obs_dim = 4  # 2 исходных признака + sin(t) + cos(t)\n",
    "act_dim = env.action_space.n  # Количество действий в среде\n",
    "\n",
    "# Копирование экспертных данных\n",
    "expert_obs = np.copy(states)  # Состояния с добавленными sin(t) и cos(t)\n",
    "expert_acts = np.copy(actions)  # Действия эксперта\n",
    "\n",
    "print(f\"Размерность состояний (obs_dim): {obs_dim}\")\n",
    "print(f\"Размерность действий (act_dim): {act_dim}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность состояний (obs_dim): 4\n",
      "Размерность действий (act_dim): 3\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Wp56QsLpcT0M",
    "ExecuteTime": {
     "end_time": "2025-05-04T17:57:19.239394Z",
     "start_time": "2025-05-04T17:57:19.235590Z"
    }
   },
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, act_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Преобразуем входные данные в тензор, если они не являются тензором\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "        logits = self.net(obs)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        # Получаем распределение действий\n",
    "        dist = self.forward(obs)\n",
    "        # Выбираем действие\n",
    "        action = dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def get_log_prob(self, obs, actions):\n",
    "        # Получаем распределение действий\n",
    "        dist = self.forward(obs)\n",
    "        # Возвращаем логарифм вероятности выбранных действий\n",
    "        return dist.log_prob(actions)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PYe2ekNUcUf5",
    "ExecuteTime": {
     "end_time": "2025-05-04T17:57:21.101597Z",
     "start_time": "2025-05-04T17:57:21.098598Z"
    }
   },
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        # Преобразуем действия в one-hot encoding\n",
    "        act_onehot = F.one_hot(act, num_classes=3).float()  # num_classes = 3 для MountainCar-v0\n",
    "        \n",
    "        # Объединяем состояние и one-hot encoded действия\n",
    "        x = torch.cat([obs, act_onehot], dim=1)\n",
    "        \n",
    "        # Пропускаем через сеть\n",
    "        return self.net(x)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cV5NtwYMccNS",
    "ExecuteTime": {
     "end_time": "2025-05-04T17:57:23.544560Z",
     "start_time": "2025-05-04T17:57:23.541185Z"
    }
   },
   "source": [
    "class TrajectoryBuffer:\n",
    "    def __init__(self):\n",
    "        # Инициализация списков для хранения данных\n",
    "        self.obs, self.acts, self.rews = [], [], []\n",
    "\n",
    "    def store(self, o, a, r):\n",
    "        # Добавление состояния, действия и награды в буфер\n",
    "        self.obs.append(o)\n",
    "        self.acts.append(a)\n",
    "        self.rews.append(r)\n",
    "\n",
    "    def get(self):\n",
    "        # Преобразование данных в тензоры PyTorch\n",
    "        obs_tensor = torch.tensor(np.array(self.obs), dtype=torch.float32)\n",
    "        acts_tensor = torch.tensor(np.array(self.acts), dtype=torch.long)\n",
    "        rews_tensor = torch.tensor(np.array(self.rews), dtype=torch.float32)\n",
    "        \n",
    "        # Очистка буфера после извлечения данных\n",
    "        self.clear()\n",
    "        \n",
    "        return obs_tensor, acts_tensor, rews_tensor\n",
    "\n",
    "    def clear(self):\n",
    "        # Очистка буфера\n",
    "        self.obs, self.acts, self.rews = [], [], []"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MxfjF-ZjceE7",
    "ExecuteTime": {
     "end_time": "2025-05-04T17:57:26.462261Z",
     "start_time": "2025-05-04T17:57:25.441129Z"
    }
   },
   "source": [
    "env = gym.make(\"MountainCar-v0\", )\n",
    "policy = Policy(obs_dim, act_dim)\n",
    "discrim = Discriminator(obs_dim, act_dim)\n",
    "\n",
    "policy_opt = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "discrim_opt = optim.Adam(discrim.parameters(), lr=1e-3)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R5rKCyC0ch0W",
    "ExecuteTime": {
     "end_time": "2025-05-04T17:59:24.484675Z",
     "start_time": "2025-05-04T17:57:29.962982Z"
    }
   },
   "source": [
    "for epoch in range(3000):\n",
    "    buf = TrajectoryBuffer()\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    t = 0  # Временная метка\n",
    "\n",
    "    while not done:\n",
    "        # Добавляем sin(t) и cos(t) к состоянию\n",
    "        augmented_obs = np.append(obs, [np.sin(t), np.cos(t)])\n",
    "        \n",
    "        # Преобразуем состояние в тензор\n",
    "        obs_tensor = torch.tensor(augmented_obs, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # Получаем действие от политики\n",
    "        action = policy.get_action(obs_tensor)\n",
    "        \n",
    "        # Совершаем шаг в среде\n",
    "        next_obs, _, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Сохраняем данные в буфер\n",
    "        buf.store(augmented_obs, action, 0)\n",
    "        \n",
    "        # Обновляем временную метку и состояние\n",
    "        t += 1\n",
    "        obs = next_obs\n",
    "\n",
    "    # Получаем данные из буфера\n",
    "    agent_obs, agent_acts, _ = buf.get()\n",
    "\n",
    "    # Выбираем случайные данные эксперта\n",
    "    idxs = np.random.choice(len(expert_obs), len(agent_obs), replace=False)\n",
    "    exp_obs = torch.tensor(expert_obs[idxs], dtype=torch.float32)\n",
    "    exp_acts = torch.tensor(expert_acts[idxs], dtype=torch.long)\n",
    "\n",
    "    # Обучение дискриминатора\n",
    "    for _ in range(2):\n",
    "        discrim_opt.zero_grad()\n",
    "\n",
    "        # Предсказания дискриминатора\n",
    "        agent_preds = discrim(agent_obs, agent_acts)\n",
    "        expert_preds = discrim(exp_obs, exp_acts)\n",
    "\n",
    "        # Бинарная кросс-энтропия\n",
    "        disc_loss = -(torch.log(expert_preds + 1e-8).mean() - torch.log(1 - agent_preds + 1e-8).mean())\n",
    "\n",
    "        disc_loss.backward()\n",
    "        discrim_opt.step()\n",
    "\n",
    "    # Получение награды от дискриминатора\n",
    "    with torch.no_grad():\n",
    "        rewards = -torch.log(1 - discrim(agent_obs, agent_acts) + 1e-8)\n",
    "\n",
    "    # Обучение политики\n",
    "    policy_opt.zero_grad()\n",
    "\n",
    "    # Логарифмы вероятностей выбранных действий\n",
    "    log_probs = policy.get_log_prob(agent_obs, agent_acts)\n",
    "\n",
    "    # Потери для стратегии\n",
    "    loss = -(rewards * log_probs).mean()\n",
    "\n",
    "    loss.backward()\n",
    "    policy_opt.step()\n",
    "\n",
    "    # Вывод информации каждые 10 эпох\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: GAIL Loss {loss.item():.3f}, Disc Loss {disc_loss.item():.3f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: GAIL Loss 0.756, Disc Loss 0.008\n",
      "Epoch 10: GAIL Loss 0.988, Disc Loss -0.387\n",
      "Epoch 20: GAIL Loss 1.300, Disc Loss -0.815\n",
      "Epoch 30: GAIL Loss 1.728, Disc Loss -1.316\n",
      "Epoch 40: GAIL Loss 2.326, Disc Loss -1.947\n",
      "Epoch 50: GAIL Loss 3.098, Disc Loss -2.749\n",
      "Epoch 60: GAIL Loss 4.076, Disc Loss -3.730\n",
      "Epoch 70: GAIL Loss 5.387, Disc Loss -4.986\n",
      "Epoch 80: GAIL Loss 7.332, Disc Loss -6.512\n",
      "Epoch 90: GAIL Loss 9.272, Disc Loss -8.375\n",
      "Epoch 100: GAIL Loss 11.577, Disc Loss -10.498\n",
      "Epoch 110: GAIL Loss 14.317, Disc Loss -12.920\n",
      "Epoch 120: GAIL Loss 17.072, Disc Loss -15.594\n",
      "Epoch 130: GAIL Loss 20.244, Disc Loss -18.421\n",
      "Epoch 140: GAIL Loss 19.981, Disc Loss -18.421\n",
      "Epoch 150: GAIL Loss 19.910, Disc Loss -18.421\n",
      "Epoch 160: GAIL Loss 20.292, Disc Loss -18.421\n",
      "Epoch 170: GAIL Loss 20.108, Disc Loss -18.421\n",
      "Epoch 180: GAIL Loss 19.904, Disc Loss -18.421\n",
      "Epoch 190: GAIL Loss 20.288, Disc Loss -18.421\n",
      "Epoch 200: GAIL Loss 20.293, Disc Loss -18.421\n",
      "Epoch 210: GAIL Loss 20.190, Disc Loss -18.421\n",
      "Epoch 220: GAIL Loss 20.155, Disc Loss -18.421\n",
      "Epoch 230: GAIL Loss 20.193, Disc Loss -18.421\n",
      "Epoch 240: GAIL Loss 20.051, Disc Loss -18.421\n",
      "Epoch 250: GAIL Loss 20.135, Disc Loss -18.421\n",
      "Epoch 260: GAIL Loss 20.167, Disc Loss -18.421\n",
      "Epoch 270: GAIL Loss 19.971, Disc Loss -18.421\n",
      "Epoch 280: GAIL Loss 19.983, Disc Loss -18.421\n",
      "Epoch 290: GAIL Loss 20.294, Disc Loss -18.421\n",
      "Epoch 300: GAIL Loss 20.065, Disc Loss -18.421\n",
      "Epoch 310: GAIL Loss 20.164, Disc Loss -18.421\n",
      "Epoch 320: GAIL Loss 20.061, Disc Loss -18.421\n",
      "Epoch 330: GAIL Loss 19.937, Disc Loss -18.421\n",
      "Epoch 340: GAIL Loss 19.926, Disc Loss -18.421\n",
      "Epoch 350: GAIL Loss 20.039, Disc Loss -18.421\n",
      "Epoch 360: GAIL Loss 20.011, Disc Loss -18.421\n",
      "Epoch 370: GAIL Loss 19.962, Disc Loss -18.421\n",
      "Epoch 380: GAIL Loss 20.255, Disc Loss -18.421\n",
      "Epoch 390: GAIL Loss 20.106, Disc Loss -18.421\n",
      "Epoch 400: GAIL Loss 20.184, Disc Loss -18.421\n",
      "Epoch 410: GAIL Loss 20.321, Disc Loss -18.421\n",
      "Epoch 420: GAIL Loss 19.932, Disc Loss -18.421\n",
      "Epoch 430: GAIL Loss 19.391, Disc Loss -18.421\n",
      "Epoch 440: GAIL Loss 20.387, Disc Loss -18.408\n",
      "Epoch 450: GAIL Loss 20.034, Disc Loss -18.421\n",
      "Epoch 460: GAIL Loss 19.628, Disc Loss -18.421\n",
      "Epoch 470: GAIL Loss 19.788, Disc Loss -18.421\n",
      "Epoch 480: GAIL Loss 19.877, Disc Loss -18.421\n",
      "Epoch 490: GAIL Loss 20.158, Disc Loss -18.421\n",
      "Epoch 500: GAIL Loss 19.522, Disc Loss -18.421\n",
      "Epoch 510: GAIL Loss 19.859, Disc Loss -18.421\n",
      "Epoch 520: GAIL Loss 19.960, Disc Loss -18.421\n",
      "Epoch 530: GAIL Loss 19.937, Disc Loss -18.421\n",
      "Epoch 540: GAIL Loss 20.057, Disc Loss -18.421\n",
      "Epoch 550: GAIL Loss 19.727, Disc Loss -18.421\n",
      "Epoch 560: GAIL Loss 19.939, Disc Loss -18.421\n",
      "Epoch 570: GAIL Loss 19.974, Disc Loss -18.421\n",
      "Epoch 580: GAIL Loss 19.737, Disc Loss -18.421\n",
      "Epoch 590: GAIL Loss 19.557, Disc Loss -18.306\n",
      "Epoch 600: GAIL Loss 19.694, Disc Loss -18.421\n",
      "Epoch 610: GAIL Loss 19.281, Disc Loss -18.421\n",
      "Epoch 620: GAIL Loss 19.301, Disc Loss -18.421\n",
      "Epoch 630: GAIL Loss 19.605, Disc Loss -18.421\n",
      "Epoch 640: GAIL Loss 19.783, Disc Loss -18.421\n",
      "Epoch 650: GAIL Loss 19.779, Disc Loss -18.421\n",
      "Epoch 660: GAIL Loss 19.979, Disc Loss -18.421\n",
      "Epoch 670: GAIL Loss 18.690, Disc Loss -18.421\n",
      "Epoch 680: GAIL Loss 19.815, Disc Loss -18.421\n",
      "Epoch 690: GAIL Loss 19.381, Disc Loss -18.421\n",
      "Epoch 700: GAIL Loss 19.487, Disc Loss -18.421\n",
      "Epoch 710: GAIL Loss 19.477, Disc Loss -18.421\n",
      "Epoch 720: GAIL Loss 19.279, Disc Loss -18.421\n",
      "Epoch 730: GAIL Loss 19.287, Disc Loss -18.421\n",
      "Epoch 740: GAIL Loss 19.657, Disc Loss -18.421\n",
      "Epoch 750: GAIL Loss 19.996, Disc Loss -18.421\n",
      "Epoch 760: GAIL Loss 19.585, Disc Loss -18.421\n",
      "Epoch 770: GAIL Loss 19.553, Disc Loss -18.421\n",
      "Epoch 780: GAIL Loss 19.640, Disc Loss -18.421\n",
      "Epoch 790: GAIL Loss 18.402, Disc Loss -18.421\n",
      "Epoch 800: GAIL Loss 19.541, Disc Loss -18.421\n",
      "Epoch 810: GAIL Loss 19.951, Disc Loss -18.421\n",
      "Epoch 820: GAIL Loss 19.367, Disc Loss -18.421\n",
      "Epoch 830: GAIL Loss 19.010, Disc Loss -18.421\n",
      "Epoch 840: GAIL Loss 19.479, Disc Loss -18.421\n",
      "Epoch 850: GAIL Loss 18.770, Disc Loss -18.421\n",
      "Epoch 860: GAIL Loss 19.785, Disc Loss -18.421\n",
      "Epoch 870: GAIL Loss 18.855, Disc Loss -18.421\n",
      "Epoch 880: GAIL Loss 18.952, Disc Loss -18.421\n",
      "Epoch 890: GAIL Loss 18.940, Disc Loss -18.421\n",
      "Epoch 900: GAIL Loss 19.249, Disc Loss -18.421\n",
      "Epoch 910: GAIL Loss 18.902, Disc Loss -18.421\n",
      "Epoch 920: GAIL Loss 17.818, Disc Loss -18.421\n",
      "Epoch 930: GAIL Loss 18.030, Disc Loss -18.421\n",
      "Epoch 940: GAIL Loss 18.813, Disc Loss -18.421\n",
      "Epoch 950: GAIL Loss 18.300, Disc Loss -18.421\n",
      "Epoch 960: GAIL Loss 19.718, Disc Loss -18.421\n",
      "Epoch 970: GAIL Loss 18.848, Disc Loss -18.421\n",
      "Epoch 980: GAIL Loss 19.618, Disc Loss -18.421\n",
      "Epoch 990: GAIL Loss 18.214, Disc Loss -18.421\n",
      "Epoch 1000: GAIL Loss 18.893, Disc Loss -18.421\n",
      "Epoch 1010: GAIL Loss 18.908, Disc Loss -18.421\n",
      "Epoch 1020: GAIL Loss 18.889, Disc Loss -18.421\n",
      "Epoch 1030: GAIL Loss 18.644, Disc Loss -18.421\n",
      "Epoch 1040: GAIL Loss 19.050, Disc Loss -18.421\n",
      "Epoch 1050: GAIL Loss 18.068, Disc Loss -18.421\n",
      "Epoch 1060: GAIL Loss 18.846, Disc Loss -18.421\n",
      "Epoch 1070: GAIL Loss 18.890, Disc Loss -18.421\n",
      "Epoch 1080: GAIL Loss 18.870, Disc Loss -18.421\n",
      "Epoch 1090: GAIL Loss 17.885, Disc Loss -18.421\n",
      "Epoch 1100: GAIL Loss 17.004, Disc Loss -18.421\n",
      "Epoch 1110: GAIL Loss 17.549, Disc Loss -18.421\n",
      "Epoch 1120: GAIL Loss 18.128, Disc Loss -18.421\n",
      "Epoch 1130: GAIL Loss 18.902, Disc Loss -18.421\n",
      "Epoch 1140: GAIL Loss 18.879, Disc Loss -18.421\n",
      "Epoch 1150: GAIL Loss 18.214, Disc Loss -18.421\n",
      "Epoch 1160: GAIL Loss 18.514, Disc Loss -18.421\n",
      "Epoch 1170: GAIL Loss 17.628, Disc Loss -18.421\n",
      "Epoch 1180: GAIL Loss 16.913, Disc Loss -18.421\n",
      "Epoch 1190: GAIL Loss 18.334, Disc Loss -18.421\n",
      "Epoch 1200: GAIL Loss 17.628, Disc Loss -18.421\n",
      "Epoch 1210: GAIL Loss 17.238, Disc Loss -18.421\n",
      "Epoch 1220: GAIL Loss 18.757, Disc Loss -18.421\n",
      "Epoch 1230: GAIL Loss 18.366, Disc Loss -18.421\n",
      "Epoch 1240: GAIL Loss 17.573, Disc Loss -18.421\n",
      "Epoch 1250: GAIL Loss 17.294, Disc Loss -18.421\n",
      "Epoch 1260: GAIL Loss 17.783, Disc Loss -18.421\n",
      "Epoch 1270: GAIL Loss 16.420, Disc Loss -18.421\n",
      "Epoch 1280: GAIL Loss 16.238, Disc Loss -18.421\n",
      "Epoch 1290: GAIL Loss 17.394, Disc Loss -18.421\n",
      "Epoch 1300: GAIL Loss 17.528, Disc Loss -18.421\n",
      "Epoch 1310: GAIL Loss 16.862, Disc Loss -18.421\n",
      "Epoch 1320: GAIL Loss 15.864, Disc Loss -18.421\n",
      "Epoch 1330: GAIL Loss 15.851, Disc Loss -18.421\n",
      "Epoch 1340: GAIL Loss 17.037, Disc Loss -18.421\n",
      "Epoch 1350: GAIL Loss 14.918, Disc Loss -18.421\n",
      "Epoch 1360: GAIL Loss 16.254, Disc Loss -18.421\n",
      "Epoch 1370: GAIL Loss 15.300, Disc Loss -18.421\n",
      "Epoch 1380: GAIL Loss 14.990, Disc Loss -18.421\n",
      "Epoch 1390: GAIL Loss 14.590, Disc Loss -18.421\n",
      "Epoch 1400: GAIL Loss 14.300, Disc Loss -18.421\n",
      "Epoch 1410: GAIL Loss 12.592, Disc Loss -18.421\n",
      "Epoch 1420: GAIL Loss 15.338, Disc Loss -18.421\n",
      "Epoch 1430: GAIL Loss 14.951, Disc Loss -18.421\n",
      "Epoch 1440: GAIL Loss 14.064, Disc Loss -18.421\n",
      "Epoch 1450: GAIL Loss 14.559, Disc Loss -18.421\n",
      "Epoch 1460: GAIL Loss 13.899, Disc Loss -18.421\n",
      "Epoch 1470: GAIL Loss 15.058, Disc Loss -18.421\n",
      "Epoch 1480: GAIL Loss 13.621, Disc Loss -18.421\n",
      "Epoch 1490: GAIL Loss 13.507, Disc Loss -18.421\n",
      "Epoch 1500: GAIL Loss 15.042, Disc Loss -18.421\n",
      "Epoch 1510: GAIL Loss 14.328, Disc Loss -18.421\n",
      "Epoch 1520: GAIL Loss 13.985, Disc Loss -18.421\n",
      "Epoch 1530: GAIL Loss 14.790, Disc Loss -18.421\n",
      "Epoch 1540: GAIL Loss 13.715, Disc Loss -18.421\n",
      "Epoch 1550: GAIL Loss 13.314, Disc Loss -18.421\n",
      "Epoch 1560: GAIL Loss 14.464, Disc Loss -18.421\n",
      "Epoch 1570: GAIL Loss 13.395, Disc Loss -18.421\n",
      "Epoch 1580: GAIL Loss 14.578, Disc Loss -18.421\n",
      "Epoch 1590: GAIL Loss 15.561, Disc Loss -18.421\n",
      "Epoch 1600: GAIL Loss 16.049, Disc Loss -18.421\n",
      "Epoch 1610: GAIL Loss 15.497, Disc Loss -18.421\n",
      "Epoch 1620: GAIL Loss 14.398, Disc Loss -18.421\n",
      "Epoch 1630: GAIL Loss 14.702, Disc Loss -18.421\n",
      "Epoch 1640: GAIL Loss 14.582, Disc Loss -18.421\n",
      "Epoch 1650: GAIL Loss 14.150, Disc Loss -18.421\n",
      "Epoch 1660: GAIL Loss 13.985, Disc Loss -18.421\n",
      "Epoch 1670: GAIL Loss 13.489, Disc Loss -18.421\n",
      "Epoch 1680: GAIL Loss 14.877, Disc Loss -18.421\n",
      "Epoch 1690: GAIL Loss 14.809, Disc Loss -18.421\n",
      "Epoch 1700: GAIL Loss 13.799, Disc Loss -18.421\n",
      "Epoch 1710: GAIL Loss 15.174, Disc Loss -18.421\n",
      "Epoch 1720: GAIL Loss 14.956, Disc Loss -18.421\n",
      "Epoch 1730: GAIL Loss 14.810, Disc Loss -18.421\n",
      "Epoch 1740: GAIL Loss 13.507, Disc Loss -18.421\n",
      "Epoch 1750: GAIL Loss 11.564, Disc Loss -18.421\n",
      "Epoch 1760: GAIL Loss 13.277, Disc Loss -18.421\n",
      "Epoch 1770: GAIL Loss 13.345, Disc Loss -18.421\n",
      "Epoch 1780: GAIL Loss 12.646, Disc Loss -18.421\n",
      "Epoch 1790: GAIL Loss 12.576, Disc Loss -18.421\n",
      "Epoch 1800: GAIL Loss 12.671, Disc Loss -18.421\n",
      "Epoch 1810: GAIL Loss 14.628, Disc Loss -18.421\n",
      "Epoch 1820: GAIL Loss 12.261, Disc Loss -18.421\n",
      "Epoch 1830: GAIL Loss 13.634, Disc Loss -18.421\n",
      "Epoch 1840: GAIL Loss 12.625, Disc Loss -18.421\n",
      "Epoch 1850: GAIL Loss 10.658, Disc Loss -18.421\n",
      "Epoch 1860: GAIL Loss 11.902, Disc Loss -18.421\n",
      "Epoch 1870: GAIL Loss 11.629, Disc Loss -18.421\n",
      "Epoch 1880: GAIL Loss 10.763, Disc Loss -18.421\n",
      "Epoch 1890: GAIL Loss 9.685, Disc Loss -18.421\n",
      "Epoch 1900: GAIL Loss 10.339, Disc Loss -18.421\n",
      "Epoch 1910: GAIL Loss 8.260, Disc Loss -18.421\n",
      "Epoch 1920: GAIL Loss 8.490, Disc Loss -18.421\n",
      "Epoch 1930: GAIL Loss 9.577, Disc Loss -18.421\n",
      "Epoch 1940: GAIL Loss 12.349, Disc Loss -18.421\n",
      "Epoch 1950: GAIL Loss 10.081, Disc Loss -18.421\n",
      "Epoch 1960: GAIL Loss 8.874, Disc Loss -18.421\n",
      "Epoch 1970: GAIL Loss 11.414, Disc Loss -18.421\n",
      "Epoch 1980: GAIL Loss 12.822, Disc Loss -18.421\n",
      "Epoch 1990: GAIL Loss 9.971, Disc Loss -18.421\n",
      "Epoch 2000: GAIL Loss 10.461, Disc Loss -18.421\n",
      "Epoch 2010: GAIL Loss 10.486, Disc Loss -18.421\n",
      "Epoch 2020: GAIL Loss 11.787, Disc Loss -18.421\n",
      "Epoch 2030: GAIL Loss 12.840, Disc Loss -18.421\n",
      "Epoch 2040: GAIL Loss 11.693, Disc Loss -18.421\n",
      "Epoch 2050: GAIL Loss 12.148, Disc Loss -18.421\n",
      "Epoch 2060: GAIL Loss 9.598, Disc Loss -18.421\n",
      "Epoch 2070: GAIL Loss 11.536, Disc Loss -18.421\n",
      "Epoch 2080: GAIL Loss 9.508, Disc Loss -18.421\n",
      "Epoch 2090: GAIL Loss 12.019, Disc Loss -18.421\n",
      "Epoch 2100: GAIL Loss 10.742, Disc Loss -18.421\n",
      "Epoch 2110: GAIL Loss 10.891, Disc Loss -18.421\n",
      "Epoch 2120: GAIL Loss 11.577, Disc Loss -18.421\n",
      "Epoch 2130: GAIL Loss 9.979, Disc Loss -18.421\n",
      "Epoch 2140: GAIL Loss 8.415, Disc Loss -18.421\n",
      "Epoch 2150: GAIL Loss 10.607, Disc Loss -18.421\n",
      "Epoch 2160: GAIL Loss 10.519, Disc Loss -18.421\n",
      "Epoch 2170: GAIL Loss 10.799, Disc Loss -18.421\n",
      "Epoch 2180: GAIL Loss 15.148, Disc Loss -18.421\n",
      "Epoch 2190: GAIL Loss 12.393, Disc Loss -18.421\n",
      "Epoch 2200: GAIL Loss 12.571, Disc Loss -18.421\n",
      "Epoch 2210: GAIL Loss 10.923, Disc Loss -18.421\n",
      "Epoch 2220: GAIL Loss 13.100, Disc Loss -18.421\n",
      "Epoch 2230: GAIL Loss 12.401, Disc Loss -18.421\n",
      "Epoch 2240: GAIL Loss 11.583, Disc Loss -18.421\n",
      "Epoch 2250: GAIL Loss 11.259, Disc Loss -18.421\n",
      "Epoch 2260: GAIL Loss 11.777, Disc Loss -18.421\n",
      "Epoch 2270: GAIL Loss 11.315, Disc Loss -18.421\n",
      "Epoch 2280: GAIL Loss 12.717, Disc Loss -18.421\n",
      "Epoch 2290: GAIL Loss 10.802, Disc Loss -18.421\n",
      "Epoch 2300: GAIL Loss 11.040, Disc Loss -18.421\n",
      "Epoch 2310: GAIL Loss 10.966, Disc Loss -18.421\n",
      "Epoch 2320: GAIL Loss 11.619, Disc Loss -18.421\n",
      "Epoch 2330: GAIL Loss 11.376, Disc Loss -18.421\n",
      "Epoch 2340: GAIL Loss 8.821, Disc Loss -18.421\n",
      "Epoch 2350: GAIL Loss 9.275, Disc Loss -18.421\n",
      "Epoch 2360: GAIL Loss 8.530, Disc Loss -18.421\n",
      "Epoch 2370: GAIL Loss 11.788, Disc Loss -18.421\n",
      "Epoch 2380: GAIL Loss 9.265, Disc Loss -18.421\n",
      "Epoch 2390: GAIL Loss 9.228, Disc Loss -18.421\n",
      "Epoch 2400: GAIL Loss 9.602, Disc Loss -18.421\n",
      "Epoch 2410: GAIL Loss 10.675, Disc Loss -18.421\n",
      "Epoch 2420: GAIL Loss 9.922, Disc Loss -18.421\n",
      "Epoch 2430: GAIL Loss 9.393, Disc Loss -18.421\n",
      "Epoch 2440: GAIL Loss 10.113, Disc Loss -18.421\n",
      "Epoch 2450: GAIL Loss 8.976, Disc Loss -18.421\n",
      "Epoch 2460: GAIL Loss 8.168, Disc Loss -18.421\n",
      "Epoch 2470: GAIL Loss 8.486, Disc Loss -18.421\n",
      "Epoch 2480: GAIL Loss 8.134, Disc Loss -18.421\n",
      "Epoch 2490: GAIL Loss 8.256, Disc Loss -18.421\n",
      "Epoch 2500: GAIL Loss 8.776, Disc Loss -18.421\n",
      "Epoch 2510: GAIL Loss 8.855, Disc Loss -18.421\n",
      "Epoch 2520: GAIL Loss 7.837, Disc Loss -18.421\n",
      "Epoch 2530: GAIL Loss 6.675, Disc Loss -18.421\n",
      "Epoch 2540: GAIL Loss 8.579, Disc Loss -18.421\n",
      "Epoch 2550: GAIL Loss 7.813, Disc Loss -18.421\n",
      "Epoch 2560: GAIL Loss 5.559, Disc Loss -18.421\n",
      "Epoch 2570: GAIL Loss 7.578, Disc Loss -18.421\n",
      "Epoch 2580: GAIL Loss 5.893, Disc Loss -18.421\n",
      "Epoch 2590: GAIL Loss 6.168, Disc Loss -18.421\n",
      "Epoch 2600: GAIL Loss 6.131, Disc Loss -18.421\n",
      "Epoch 2610: GAIL Loss 6.148, Disc Loss -18.421\n",
      "Epoch 2620: GAIL Loss 5.900, Disc Loss -18.421\n",
      "Epoch 2630: GAIL Loss 6.010, Disc Loss -18.421\n",
      "Epoch 2640: GAIL Loss 4.916, Disc Loss -18.421\n",
      "Epoch 2650: GAIL Loss 4.887, Disc Loss -18.421\n",
      "Epoch 2660: GAIL Loss 5.522, Disc Loss -18.421\n",
      "Epoch 2670: GAIL Loss 4.381, Disc Loss -18.421\n",
      "Epoch 2680: GAIL Loss 7.818, Disc Loss -18.421\n",
      "Epoch 2690: GAIL Loss 5.125, Disc Loss -18.421\n",
      "Epoch 2700: GAIL Loss 5.726, Disc Loss -18.421\n",
      "Epoch 2710: GAIL Loss 5.706, Disc Loss -18.421\n",
      "Epoch 2720: GAIL Loss 6.410, Disc Loss -18.421\n",
      "Epoch 2730: GAIL Loss 7.984, Disc Loss -18.421\n",
      "Epoch 2740: GAIL Loss 8.033, Disc Loss -18.421\n",
      "Epoch 2750: GAIL Loss 6.123, Disc Loss -18.421\n",
      "Epoch 2760: GAIL Loss 7.886, Disc Loss -18.421\n",
      "Epoch 2770: GAIL Loss 5.964, Disc Loss -18.421\n",
      "Epoch 2780: GAIL Loss 6.469, Disc Loss -18.421\n",
      "Epoch 2790: GAIL Loss 5.093, Disc Loss -18.421\n",
      "Epoch 2800: GAIL Loss 5.851, Disc Loss -18.421\n",
      "Epoch 2810: GAIL Loss 6.888, Disc Loss -18.421\n",
      "Epoch 2820: GAIL Loss 6.249, Disc Loss -18.421\n",
      "Epoch 2830: GAIL Loss 6.339, Disc Loss -18.421\n",
      "Epoch 2840: GAIL Loss 5.281, Disc Loss -18.421\n",
      "Epoch 2850: GAIL Loss 4.606, Disc Loss -18.421\n",
      "Epoch 2860: GAIL Loss 5.960, Disc Loss -18.421\n",
      "Epoch 2870: GAIL Loss 6.211, Disc Loss -18.421\n",
      "Epoch 2880: GAIL Loss 7.506, Disc Loss -18.421\n",
      "Epoch 2890: GAIL Loss 6.089, Disc Loss -18.421\n",
      "Epoch 2900: GAIL Loss 6.190, Disc Loss -18.421\n",
      "Epoch 2910: GAIL Loss 6.123, Disc Loss -18.421\n",
      "Epoch 2920: GAIL Loss 4.927, Disc Loss -18.421\n",
      "Epoch 2930: GAIL Loss 3.576, Disc Loss -18.421\n",
      "Epoch 2940: GAIL Loss 4.854, Disc Loss -18.421\n",
      "Epoch 2950: GAIL Loss 4.161, Disc Loss -18.421\n",
      "Epoch 2960: GAIL Loss 4.296, Disc Loss -18.421\n",
      "Epoch 2970: GAIL Loss 6.870, Disc Loss -18.421\n",
      "Epoch 2980: GAIL Loss 3.949, Disc Loss -18.421\n",
      "Epoch 2990: GAIL Loss 4.794, Disc Loss -18.421\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zu1Cjaikdwg7"
   },
   "source": [
    "Протестируйте ваш алгоритм"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Mm4v4361UcSS",
    "ExecuteTime": {
     "end_time": "2025-05-04T18:09:10.419550Z",
     "start_time": "2025-05-04T18:09:10.120507Z"
    }
   },
   "source": [
    "for episode in range(10):\n",
    "    obs, _ = env.reset()  # Получаем начальное состояние (игнорируем info)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    t = 0  # Временная метка\n",
    "\n",
    "    while not done:\n",
    "        # Добавляем sin(t) и cos(t) к состоянию\n",
    "        augmented_obs = np.append(obs, [np.sin(t), np.cos(t)])\n",
    "        \n",
    "        # Преобразуем состояние в тензор\n",
    "        obs_tensor = torch.tensor(augmented_obs, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # Получаем действие от политики\n",
    "        action = policy.get_action(obs_tensor)\n",
    "        \n",
    "        # Совершаем шаг в среде\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Обновляем состояние и награду\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        t += 1\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "env.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -200.0\n",
      "Episode 2: Total Reward = -200.0\n",
      "Episode 3: Total Reward = -200.0\n",
      "Episode 4: Total Reward = -200.0\n",
      "Episode 5: Total Reward = -200.0\n",
      "Episode 6: Total Reward = -200.0\n",
      "Episode 7: Total Reward = -200.0\n",
      "Episode 8: Total Reward = -200.0\n",
      "Episode 9: Total Reward = -200.0\n",
      "Episode 10: Total Reward = -200.0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNbV8AuXc57amLtPBcU7duA",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
